Title: 性能测试随想
Date: 2012-11-13 21:12
Category: 性能测试与调优
Slug: performance-test-tuning

## 分类

**性能测试：** 系统常规状态运行时，关注机器资源耗用、响应时间等指标。（可能需要长时间保持系统这个状态，观察是否会有资源泄漏）

**容量测试：** 关注系统单位时间内能够处理的最大请求数

**过载测试：** 关注系统过载时系统能够提供的服务。理想的情况是系统仍然可以提供自身容量的服务。

**峰谷测试：** 关注系统从高负载恢复、转为几乎空闲、然后再攀升到高负载、再降低的能力。比如可以观察第二个高负载和第一个高负载时期系统性能指标是否有差异、在负载强烈波动的情况下是或否会有资源泄漏等。

类似的场景还有很多，个人觉得具体还要看被测系统在实际的业务中是否会出现这些场景，不一定要覆盖所有的性能测试子分类。上面列出来的都是我接触过的一些测试场景。


## 性能测试常见的关注指标：

**业务指标：**吞吐量（Throughput）和响应时间（或者叫延迟Latency）

**系统资源：**CPU空闲率、内存使用、网络IO、磁盘读写量、句柄数等等

**并发数、QPS、吞吐量的关系：**

**并发数：**有多少用户同时访问我们的服务，注意这并不代表这么多用户一直在请求我们的服务，因为：

1. 一般用户会等待服务的响应之后才会发起下一次请求 
2. 用户多个操作之间存在“思考时间”

**QPS：**是指服务器实际每秒处理的请求数，和吞吐量稍不同的是，它是一个实际值，可能会随并发数变化而变化。

**吞吐量：**指单位时间内服务器可以处理完成的最大请求数，即指QPS的最大值。

**吞吐量理论值**=并发数/Latency。
为什么说是理论值，因为这个吞吐量只是根据并发数和延迟的制约算出来的值。实际上这个值还受其他资源限制，比如CPU和网络 IO。我遇到过一个服务响应时间15ms左右，开启30线程，吞吐量理论值可以达到30/0.015=2000qps，但是在系统达到1000qps左右的时候，机器的cpu idle只有40%左右了，很明显这个系统在这台机器上吞吐量达不到2000。

**如何设定服务器的工作线程数：**

我们设计多线程程序，实际是为了充分利用CPU。单线程只能运行在单核上面。所以为了充分利用CPU，工作线程数至少设置为CPU核数。

再后来我们发现，线程在处理请求的时候，可能会有一段时间处于IO或者sleep状态，这个时候CPU又是空闲的了。因此，很容易想象，假设处理一条请求的过程中，占用CPU的时间为X，处于IO或sleep等不占用CPU的时间为Y，为了充分利用CPU，我们可以设置线程数为X+Y/X（不考虑线程切换的时间）。

要精确计算出线程处理一条请求过程中占用和不占用CPU的时间还是比较困难的，比如需要等待锁的时间就不好计算。简单计算的话，可以把网络IO的时间（网络延迟）考虑进来即可。

因此，比较合适的设置为：服务器工作线程数=CPU核数\*（网络延迟+计算延迟）/计算延迟

## 系统资源查看：

### CPU空闲率

**top**
```bash
Cpu(s): 0.0% us, 0.0% sy, 0.0% ni, 100.0%id, 0.0% wa, 0.0% hi, 0.0% si
```

其中红色的部分为系统CPU的空闲率（注意，打开top命令刚开始的第一条数据可能不准确）

**sar -u 1 3**

%idle这一列的数值代表系统CPU的空闲率

### 内存使用

**top**

![image](http://7xo7ae.com1.z0.glb.clouddn.com/performance_test1.jpg)

top可以用来查看一个进程占用的虚存和实存。上图中bdbs使用的虚存为3959MB，使用的实存为3.9GB。

使用top看一个进程的虚存和实存使用，有个缺点是不够精确。如果需要精确，可以查看/proc/\$pid/status文件，其中VmSize是虚存，VmRSS是实存。

**free**

![image](http://7xo7ae.com1.z0.glb.clouddn.com/performance_test2.jpg)

free(-m表示以MB为单位)可以查看整个系统的空闲内存。上图中系统未使用的内存为5047MB，被用作buffer/cache的内存分别为922/5684MB，总的空闲内存为三部分之后，为11654MB（第二行最后一列的数值）

### 网络IO

**sar –n DEV 1 3**

![image](http://7xo7ae.com1.z0.glb.clouddn.com/performance_test3.png)

其中IFACE列表示系统的网卡链接，lo代表localhost。

rxpck/s和txpck/s：该网卡的每秒收发数据包数；rxbyt/s和txbyt/s：该网卡每秒收发字节数；rxcmp/s和txcmp/s：每秒钟接收的压缩数据包数；rxmcst/s：每秒接受的多播数据包数。

### 磁盘读写

**iostat –x**

![image](http://7xo7ae.com1.z0.glb.clouddn.com/performance_test4.png)

- rrqm/s和wrqpm:每秒进行 merge 的读和写的次数
- r/s和w/s：每秒钟读和写的次数
- rsec/s和wsec/s：每秒钟读和写扇区的个数
- rkB/s和wkB/s：每秒钟读和写的千字节数
- avgrq-sz：平均每次IO操作的扇区数
- avgqu-sz：平均I/O队列长度
- await：平均每次设备I/O操作的等待时间
- svctm: 平均每次设备I/O操作的服务时间
- %util：一秒中有多少时间 I/O 队列是非空的

### 句柄数

`lsof -n|awk '{print $2}'|sort|uniq -c|grep <pid>` 有些系统lsof只允许root用户运行，可以这样查看`ls –l /proc/<pid>/fd |wc –l`

`lsof -n|awk '{print \$2}'|sort|uniq -c|sort -nr|more`可以查看所有进程打开的句柄数。

（注：lsof的方式列出了程序使用的动态库，但`/proc/<pid>/fd`下面则没有，所以两种统计结果可能稍有不同）

